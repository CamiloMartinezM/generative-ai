%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}

\usepackage[preprint,nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{siunitx}

\usepackage{csquotes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Week 1 Assignment\\
\vspace{2mm}
\small{Generative AI}
\\
\vspace{2mm}
\small{Saarland University -- Winter Semester 2024/25}
}

\author{%
  Mart√≠nez \\
  7057573 \\
  \texttt{cama00005@stud.uni-saarland.de} \\
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise Assignment: E1}
We can express an upper bound on the number of parameters required for an $n$-gram model as follows: the number of possible $2$-grams is $V^2$, and the number of possible $4$-grams is $V^4$~\cite{jm3}. Thus, the number of parameters required for an $n$-gram model is $V^n$. This is because the model needs to store the probabilities of all possible $n$-grams, and the number of possible $n$-grams is $V^n$, where $V$ is the size of the vocabulary.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise Assignment: E2}
Following the above, for a $1$-grams with $V = 50{,}000$, the number of parameters required is $V^n = 50{,}000^1 = 50{,}000$. Similarly, for a $3$-grams, the number of parameters required is $50{,}000^3 = $ \num{125e12}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise Assignment: I5}
Regardless of $n$, the generated sentences training with the corpus \texttt{processed\_berkeley\_restaurant.txt} contain less punctuation marks (e.g., \texttt{,}, \texttt{.}, \texttt{!}, \texttt{;}, etc.) than the sentences generated by training with the corpus \texttt{processed\_shakespeare.txt}. This is probably due to the fact that the latter by Shakespeare uses more punctuation marks than the former, and we are training and generating the sentences based on that respective corpus. For instance, the following is a sentence from \texttt{processed\_shakespeare.txt} generated with $n = 1$:

\begin{displayquote}
    \texttt{<s> the. ,, gracious of </s>}
\end{displayquote}

whereas the following is a sentence from \texttt{processed\_berkeley\_restaurant.txt} with $n = 3$:

\begin{displayquote}
    \texttt{<s> it should not cost more than six dollars each meal </s>}
\end{displayquote}

On the other hand, regardless of the corpus, the generated sentences with $n = 3$ are longer than those with $n = 1$, and the ones with $n = 3$ seem to be more coherent than those with $n = 1$. The previous example also exemplifies this behavior. The better \textit{coherence} is naturally due to the fact that the $3$-gram model has more context to generate the next word, namely, the $2$ previous words, which makes the generated sentences in general more prone to ``sound'' coherent. Note the quotation marks around the word \textit{sound}, as most of the generated sentences are \textit{readable} and \textit{human-like}, but they do not necessarily make sense semantically. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\section*{Acknowledgements}
Chapter 3 from the book by Jurafsky and Martin ~\cite{jm3}, provided as this week's reading assignment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{unsrt}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%