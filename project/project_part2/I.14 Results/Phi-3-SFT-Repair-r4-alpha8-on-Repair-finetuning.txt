ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
2025-02-04 22:15:51.146072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738707351.166363   22412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738707351.177197   22412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 22:15:51.199277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.1.8: Fast Mistral patching. Transformers: 4.47.1.
   \\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.1.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
GPU = Tesla T4. Max memory = 14.74127197265625 GB.
2.201 GB of memory reserved.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 957 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 4
\        /    Total batch size = 8 | Total steps = 119
 "-____-"     Number of trainable parameters = 7,471,104
{'loss': 8.4367, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.01}
{'loss': 7.9684, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.02}
{'loss': 7.5919, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.03}
{'loss': 7.8291, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.03}
{'loss': 8.3114, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.04}
{'loss': 8.3169, 'grad_norm': 79.9958724975586, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 8.6985, 'grad_norm': nan, 'learning_rate': 4e-05, 'epoch': 0.06}
{'loss': 7.8027, 'grad_norm': 88.744140625, 'learning_rate': 8e-05, 'epoch': 0.07}
{'loss': 8.4643, 'grad_norm': 41.21919631958008, 'learning_rate': 0.00012, 'epoch': 0.08}
{'loss': 7.7077, 'grad_norm': 35.32944107055664, 'learning_rate': 0.00016, 'epoch': 0.08}
{'loss': 7.7591, 'grad_norm': 80.61095428466797, 'learning_rate': 0.0002, 'epoch': 0.09}
{'loss': 6.11, 'grad_norm': nan, 'learning_rate': 0.0002, 'epoch': 0.1}
{'loss': 6.2369, 'grad_norm': 71.56129455566406, 'learning_rate': 0.00019824561403508772, 'epoch': 0.11}
{'loss': 4.693, 'grad_norm': 102.03229522705078, 'learning_rate': 0.00019649122807017543, 'epoch': 0.12}
{'loss': 3.0937, 'grad_norm': 44.07120895385742, 'learning_rate': 0.00019473684210526317, 'epoch': 0.13}
{'loss': 1.6315, 'grad_norm': 134.03855895996094, 'learning_rate': 0.00019298245614035088, 'epoch': 0.13}
{'loss': 1.0908, 'grad_norm': 19.584144592285156, 'learning_rate': 0.0001912280701754386, 'epoch': 0.14}
{'loss': 0.7318, 'grad_norm': 17.05632781982422, 'learning_rate': 0.00018947368421052632, 'epoch': 0.15}
{'loss': 0.6329, 'grad_norm': 5.33699893951416, 'learning_rate': 0.00018771929824561406, 'epoch': 0.16}
{'loss': 0.5665, 'grad_norm': 3.22096848487854, 'learning_rate': 0.00018596491228070177, 'epoch': 0.17}
{'loss': 0.4925, 'grad_norm': 3.000652313232422, 'learning_rate': 0.00018421052631578948, 'epoch': 0.18}
{'loss': 0.4749, 'grad_norm': 2.5754451751708984, 'learning_rate': 0.0001824561403508772, 'epoch': 0.18}
{'loss': 0.4381, 'grad_norm': 2.135498523712158, 'learning_rate': 0.00018070175438596493, 'epoch': 0.19}
{'loss': 0.4064, 'grad_norm': 1.8456705808639526, 'learning_rate': 0.00017894736842105264, 'epoch': 0.2}
{'loss': 0.3996, 'grad_norm': 1.3277757167816162, 'learning_rate': 0.00017719298245614035, 'epoch': 0.21}
{'loss': 0.3789, 'grad_norm': 1.0624953508377075, 'learning_rate': 0.00017543859649122806, 'epoch': 0.22}
{'loss': 0.3703, 'grad_norm': 0.23041674494743347, 'learning_rate': 0.0001736842105263158, 'epoch': 0.23}
{'loss': 0.3574, 'grad_norm': 0.2864198088645935, 'learning_rate': 0.00017192982456140353, 'epoch': 0.23}
{'loss': 0.3471, 'grad_norm': 0.48587143421173096, 'learning_rate': 0.00017017543859649124, 'epoch': 0.24}
{'loss': 0.3553, 'grad_norm': 0.4620460867881775, 'learning_rate': 0.00016842105263157895, 'epoch': 0.25}
{'loss': 0.3356, 'grad_norm': 0.4405384063720703, 'learning_rate': 0.0001666666666666667, 'epoch': 0.26}
{'loss': 0.3159, 'grad_norm': 0.47729313373565674, 'learning_rate': 0.0001649122807017544, 'epoch': 0.27}
{'loss': 0.3158, 'grad_norm': 0.3049728572368622, 'learning_rate': 0.0001631578947368421, 'epoch': 0.28}
{'loss': 0.3104, 'grad_norm': 0.2590196132659912, 'learning_rate': 0.00016140350877192982, 'epoch': 0.28}
{'loss': 0.292, 'grad_norm': 0.2947438061237335, 'learning_rate': 0.00015964912280701756, 'epoch': 0.29}
{'loss': 0.3038, 'grad_norm': 0.2460111677646637, 'learning_rate': 0.00015789473684210527, 'epoch': 0.3}
{'loss': 0.2825, 'grad_norm': 0.2424551397562027, 'learning_rate': 0.00015614035087719297, 'epoch': 0.31}
{'loss': 0.2765, 'grad_norm': 0.37210726737976074, 'learning_rate': 0.0001543859649122807, 'epoch': 0.32}
{'loss': 0.2419, 'grad_norm': 0.35076063871383667, 'learning_rate': 0.00015263157894736845, 'epoch': 0.33}
{'loss': 0.2599, 'grad_norm': 0.188142329454422, 'learning_rate': 0.00015087719298245616, 'epoch': 0.33}
{'loss': 0.2416, 'grad_norm': 0.2069036364555359, 'learning_rate': 0.00014912280701754387, 'epoch': 0.34}
{'loss': 0.2176, 'grad_norm': 0.23535187542438507, 'learning_rate': 0.00014736842105263158, 'epoch': 0.35}
{'loss': 0.2194, 'grad_norm': 0.21424119174480438, 'learning_rate': 0.00014561403508771932, 'epoch': 0.36}
{'loss': 0.2015, 'grad_norm': 0.15115487575531006, 'learning_rate': 0.00014385964912280703, 'epoch': 0.37}
{'loss': 0.181, 'grad_norm': 0.36093398928642273, 'learning_rate': 0.00014210526315789474, 'epoch': 0.38}
{'loss': 0.1727, 'grad_norm': 0.2151152789592743, 'learning_rate': 0.00014035087719298245, 'epoch': 0.38}
{'loss': 0.1742, 'grad_norm': 0.12228298932313919, 'learning_rate': 0.00013859649122807018, 'epoch': 0.39}
{'loss': 0.151, 'grad_norm': 0.0821518823504448, 'learning_rate': 0.0001368421052631579, 'epoch': 0.4}
{'loss': 0.1681, 'grad_norm': 0.1210775300860405, 'learning_rate': 0.00013508771929824563, 'epoch': 0.41}
{'loss': 0.1638, 'grad_norm': 0.1366875022649765, 'learning_rate': 0.00013333333333333334, 'epoch': 0.42}
{'loss': 0.1658, 'grad_norm': 0.11988847702741623, 'learning_rate': 0.00013157894736842108, 'epoch': 0.43}
{'loss': 0.1551, 'grad_norm': 0.10306062549352646, 'learning_rate': 0.0001298245614035088, 'epoch': 0.43}
{'loss': 0.1324, 'grad_norm': 0.10106752812862396, 'learning_rate': 0.0001280701754385965, 'epoch': 0.44}
{'loss': 0.1348, 'grad_norm': 0.10630252957344055, 'learning_rate': 0.0001263157894736842, 'epoch': 0.45}
{'loss': 0.1344, 'grad_norm': 0.1224716454744339, 'learning_rate': 0.00012456140350877194, 'epoch': 0.46}
{'loss': 0.1241, 'grad_norm': 0.10635944455862045, 'learning_rate': 0.00012280701754385965, 'epoch': 0.47}
{'loss': 0.1316, 'grad_norm': 0.12371275573968887, 'learning_rate': 0.00012105263157894738, 'epoch': 0.48}
{'loss': 0.1221, 'grad_norm': 0.11429212242364883, 'learning_rate': 0.00011929824561403509, 'epoch': 0.48}
{'loss': 0.1133, 'grad_norm': 0.14140111207962036, 'learning_rate': 0.00011754385964912282, 'epoch': 0.49}
{'loss': 0.12, 'grad_norm': 0.0821443498134613, 'learning_rate': 0.00011578947368421053, 'epoch': 0.5}
{'loss': 0.1116, 'grad_norm': 0.09712245315313339, 'learning_rate': 0.00011403508771929824, 'epoch': 0.51}
{'loss': 0.0971, 'grad_norm': 0.12953779101371765, 'learning_rate': 0.00011228070175438597, 'epoch': 0.52}
{'loss': 0.1124, 'grad_norm': 0.08455098420381546, 'learning_rate': 0.0001105263157894737, 'epoch': 0.53}
{'loss': 0.0871, 'grad_norm': 0.10808999091386795, 'learning_rate': 0.00010877192982456141, 'epoch': 0.53}
{'loss': 0.1085, 'grad_norm': 0.10904021561145782, 'learning_rate': 0.00010701754385964912, 'epoch': 0.54}
{'loss': 0.0957, 'grad_norm': 0.12116079032421112, 'learning_rate': 0.00010526315789473685, 'epoch': 0.55}
{'loss': 0.0939, 'grad_norm': 0.1294376105070114, 'learning_rate': 0.00010350877192982457, 'epoch': 0.56}
{'loss': 0.0893, 'grad_norm': 0.11354836821556091, 'learning_rate': 0.0001017543859649123, 'epoch': 0.57}
{'loss': 0.072, 'grad_norm': 0.10225126147270203, 'learning_rate': 0.0001, 'epoch': 0.58}
{'loss': 0.0718, 'grad_norm': 0.09316399693489075, 'learning_rate': 9.824561403508771e-05, 'epoch': 0.58}
{'loss': 0.0492, 'grad_norm': 0.07022073119878769, 'learning_rate': 9.649122807017544e-05, 'epoch': 0.59}
{'loss': 0.0667, 'grad_norm': 0.12112560123205185, 'learning_rate': 9.473684210526316e-05, 'epoch': 0.6}
{'loss': 0.0847, 'grad_norm': 0.11196810007095337, 'learning_rate': 9.298245614035089e-05, 'epoch': 0.61}
{'loss': 0.0544, 'grad_norm': 0.0992450937628746, 'learning_rate': 9.12280701754386e-05, 'epoch': 0.62}
{'loss': 0.0645, 'grad_norm': 0.15561076998710632, 'learning_rate': 8.947368421052632e-05, 'epoch': 0.63}
{'loss': 0.0621, 'grad_norm': 0.24675704538822174, 'learning_rate': 8.771929824561403e-05, 'epoch': 0.63}
{'loss': 0.0657, 'grad_norm': 0.07954604178667068, 'learning_rate': 8.596491228070177e-05, 'epoch': 0.64}
{'loss': 0.0635, 'grad_norm': 0.06478973478078842, 'learning_rate': 8.421052631578948e-05, 'epoch': 0.65}
{'loss': 0.0888, 'grad_norm': 0.08719577640295029, 'learning_rate': 8.24561403508772e-05, 'epoch': 0.66}
{'loss': 0.0486, 'grad_norm': 0.05636832118034363, 'learning_rate': 8.070175438596491e-05, 'epoch': 0.67}
{'loss': 0.0672, 'grad_norm': 0.07565080374479294, 'learning_rate': 7.894736842105263e-05, 'epoch': 0.68}
{'loss': 0.0584, 'grad_norm': 0.05463382974267006, 'learning_rate': 7.719298245614036e-05, 'epoch': 0.68}
{'loss': 0.0376, 'grad_norm': 0.05979913845658302, 'learning_rate': 7.543859649122808e-05, 'epoch': 0.69}
{'loss': 0.0626, 'grad_norm': 0.052742037922143936, 'learning_rate': 7.368421052631579e-05, 'epoch': 0.7}
{'loss': 0.06, 'grad_norm': 0.05367782339453697, 'learning_rate': 7.192982456140351e-05, 'epoch': 0.71}
{'loss': 0.0513, 'grad_norm': 0.05687597766518593, 'learning_rate': 7.017543859649122e-05, 'epoch': 0.72}
{'loss': 0.0674, 'grad_norm': 0.05616208538413048, 'learning_rate': 6.842105263157895e-05, 'epoch': 0.73}
{'loss': 0.054, 'grad_norm': 0.05538776144385338, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.73}
{'loss': 0.0457, 'grad_norm': 0.04910352826118469, 'learning_rate': 6.49122807017544e-05, 'epoch': 0.74}
{'loss': 0.06, 'grad_norm': 0.05090014263987541, 'learning_rate': 6.31578947368421e-05, 'epoch': 0.75}
{'loss': 0.0468, 'grad_norm': 0.05263384431600571, 'learning_rate': 6.140350877192983e-05, 'epoch': 0.76}
{'loss': 0.0625, 'grad_norm': 0.053081829100847244, 'learning_rate': 5.9649122807017544e-05, 'epoch': 0.77}
{'loss': 0.0644, 'grad_norm': 0.05493847653269768, 'learning_rate': 5.789473684210527e-05, 'epoch': 0.78}
{'loss': 0.0472, 'grad_norm': 0.0578780397772789, 'learning_rate': 5.6140350877192984e-05, 'epoch': 0.78}
{'loss': 0.0543, 'grad_norm': 0.049302179366350174, 'learning_rate': 5.438596491228071e-05, 'epoch': 0.79}
{'loss': 0.0671, 'grad_norm': 0.0558825321495533, 'learning_rate': 5.2631578947368424e-05, 'epoch': 0.8}
{'loss': 0.0565, 'grad_norm': 0.04838567227125168, 'learning_rate': 5.087719298245615e-05, 'epoch': 0.81}
{'loss': 0.0601, 'grad_norm': 0.06180839613080025, 'learning_rate': 4.912280701754386e-05, 'epoch': 0.82}
{'loss': 0.0553, 'grad_norm': 0.04868892952799797, 'learning_rate': 4.736842105263158e-05, 'epoch': 0.83}
{'loss': 0.0695, 'grad_norm': 0.053086064755916595, 'learning_rate': 4.56140350877193e-05, 'epoch': 0.84}
{'loss': 0.0626, 'grad_norm': 0.058295950293540955, 'learning_rate': 4.3859649122807014e-05, 'epoch': 0.84}
{'loss': 0.0563, 'grad_norm': 0.05244799703359604, 'learning_rate': 4.210526315789474e-05, 'epoch': 0.85}
{'loss': 0.0465, 'grad_norm': 0.0544673316180706, 'learning_rate': 4.0350877192982455e-05, 'epoch': 0.86}
{'loss': 0.044, 'grad_norm': 0.051354922354221344, 'learning_rate': 3.859649122807018e-05, 'epoch': 0.87}
{'loss': 0.0437, 'grad_norm': 0.05762346461415291, 'learning_rate': 3.6842105263157895e-05, 'epoch': 0.88}
{'loss': 0.0484, 'grad_norm': 0.04966531693935394, 'learning_rate': 3.508771929824561e-05, 'epoch': 0.89}
{'loss': 0.042, 'grad_norm': 0.05306636542081833, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.89}
{'loss': 0.0459, 'grad_norm': 0.05422637611627579, 'learning_rate': 3.157894736842105e-05, 'epoch': 0.9}
{'loss': 0.0607, 'grad_norm': 0.059844695031642914, 'learning_rate': 2.9824561403508772e-05, 'epoch': 0.91}
{'loss': 0.0534, 'grad_norm': 0.05660431087017059, 'learning_rate': 2.8070175438596492e-05, 'epoch': 0.92}
{'loss': 0.0651, 'grad_norm': 0.05952261760830879, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.93}
{'loss': 0.0538, 'grad_norm': 0.054096709936857224, 'learning_rate': 2.456140350877193e-05, 'epoch': 0.94}
{'loss': 0.0608, 'grad_norm': 0.06049404293298721, 'learning_rate': 2.280701754385965e-05, 'epoch': 0.94}
{'loss': 0.0539, 'grad_norm': 0.056456200778484344, 'learning_rate': 2.105263157894737e-05, 'epoch': 0.95}
{'loss': 0.0553, 'grad_norm': 0.05558537319302559, 'learning_rate': 1.929824561403509e-05, 'epoch': 0.96}
{'loss': 0.0509, 'grad_norm': 0.05749940499663353, 'learning_rate': 1.7543859649122806e-05, 'epoch': 0.97}
{'loss': 0.0477, 'grad_norm': 0.056827470660209656, 'learning_rate': 1.5789473684210526e-05, 'epoch': 0.98}
{'loss': 0.059, 'grad_norm': 0.05769280344247818, 'learning_rate': 1.4035087719298246e-05, 'epoch': 0.99}
{'loss': 0.0452, 'grad_norm': 0.0573805570602417, 'learning_rate': 1.2280701754385964e-05, 'epoch': 0.99}
{'train_runtime': 2547.7383, 'train_samples_per_second': 0.376, 'train_steps_per_second': 0.047, 'train_loss': 1.072136324431215, 'epoch': 0.99}
100% 119/119 [42:27<00:00, 21.41s/it]
Training runtime: 42.46 minutes
Training memory: 0.846 GB
